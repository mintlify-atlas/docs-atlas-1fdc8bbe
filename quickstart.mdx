---
title: Quickstart
description: Train your first neural network and make predictions on handwritten digits in minutes
---

## Overview

This guide will walk you through training a neural network on the MNIST dataset and using it to classify handwritten digits. By the end, you'll have a working model with 97% accuracy.

<Note>
  Make sure you've completed the [installation steps](/installation) before proceeding.
</Note>

## Training the network

Training the neural network is straightforward. The network will train for 5 epochs on the MNIST training dataset.

### Run training

```bash
./neural-network -mnist=train
```

### What happens during training

<Steps>
  <Step title="Network initialization">
    The network is created with the following architecture:
    
    ```go
    net := perceptron.CreateNetwork(784, 200, 10, 0.1)
    ```
    
    - **784 inputs**: One for each pixel in a 28×28 image
    - **200 hidden neurons**: An arbitrary number that works well for this task
    - **10 outputs**: One for each digit (0-9)
    - **0.1 learning rate**: Controls how quickly the network learns
  </Step>
  
  <Step title="Data loading and normalization">
    For each training example:
    
    ```go
    // Normalize pixel values from 0-255 to 0.01-0.99
    inputs[i] = (x / 255.0 * 0.99) + 0.01
    
    // Create target vector (all 0.01 except the correct digit)
    targets[correctDigit] = 0.99
    ```
  </Step>
  
  <Step title="Training loop">
    The network trains for 5 epochs, processing each image:
    
    ```go
    for epochs := 0; epochs < 5; epochs++ {
        fmt.Println("Epoch ", epochs)
        // Process each training example
        net.Train(inputs, targets)
    }
    ```
    
    You'll see output like:
    ```
    Epoch 0
    Epoch 1
    Epoch 2
    Epoch 3
    Epoch 4
    Time taken to train: 2m30s
    ```
  </Step>
  
  <Step title="Model saving">
    After training completes, the model weights are automatically saved:
    
    - `data/hweights.model`: Hidden layer weights
    - `data/oweights.model`: Output layer weights
  </Step>
</Steps>

<Warning>
  Training can take several minutes depending on your hardware. The MNIST training set contains 60,000 examples.
</Warning>

## Evaluating accuracy

After training, evaluate the network's performance on the test set:

```bash
./neural-network -mnist=predict
```

### Understanding the evaluation

The prediction process:

```go
outputs := net.Predict(inputs)

// Find the neuron with the highest activation
best := 0
highest := 0.0
for i := 0; i < net.Outputs; i++ {
    if outputs.At(i, 0) > highest {
        best = i
        highest = outputs.At(i, 0)
    }
}
```

You'll see output like:

```
Time taken to check: 15s
score: 9700
```

This means the network correctly classified 9,700 out of 10,000 test images (97% accuracy).

## Making predictions

You can use the trained network to classify your own handwritten digit images.

### Prepare an image

Create a 28×28 pixel PNG image of a handwritten digit. The image should:

- Be grayscale or color (will be converted to grayscale)
- Have the digit drawn in black on a white background
- Be exactly 28×28 pixels

<Note>
  The network expects images in the same format as MNIST: dark digits on a light background. The code automatically inverts pixel values to match this.
</Note>

### Run prediction

```bash
./neural-network -file=path/to/your/image.png
```

Example:

```bash
./neural-network -file=nums/3.png
```

### Prediction output

The program will:

1. Display the image (on iTerm2 terminals)
2. Show the confidence scores for each digit:
   ```
   [0.01]  # Confidence for digit 0
   [0.02]  # Confidence for digit 1
   [0.01]  # Confidence for digit 2
   [0.95]  # Confidence for digit 3 (highest)
   [0.03]  # Confidence for digit 4
   ...
   ```
3. Print the final prediction:
   ```
   prediction: 3
   ```

### How prediction works

The prediction pipeline:

<Steps>
  <Step title="Load and process image">
    ```go
    // Read PNG file and convert to grayscale
    img, err := png.Decode(imgFile)
    gray := image.NewGray(bounds)
    ```
  </Step>
  
  <Step title="Normalize pixels">
    ```go
    // Convert to same format as MNIST (inverted and normalized)
    pixels[i] = (float64(255-gray.Pix[i]) / 255.0 * 0.99) + 0.01
    ```
  </Step>
  
  <Step title="Run forward propagation">
    ```go
    output := net.Predict(pixels)
    ```
  </Step>
  
  <Step title="Find highest confidence">
    ```go
    best := 0
    highest := 0.0
    for i := 0; i < 10; i++ {
        if output.At(i, 0) > highest {
            best = i
            highest = output.At(i, 0)
        }
    }
    ```
  </Step>
</Steps>

## Complete example workflow

Here's a complete workflow from training to prediction:

<CodeGroup>

```bash Train
# Train the network (takes a few minutes)
./neural-network -mnist=train

# Output:
# Epoch 0
# Epoch 1
# Epoch 2
# Epoch 3
# Epoch 4
# Time taken to train: 2m30s
```

```bash Evaluate
# Evaluate on test set
./neural-network -mnist=predict

# Output:
# Time taken to check: 15s
# score: 9700
```

```bash Predict
# Predict a single image
./neural-network -file=nums/7.png

# Output:
# [matrix output showing confidences]
# prediction: 7
```

</CodeGroup>

## Understanding the network code

### Creating a network

The network is initialized with random weights:

```go
net := perceptron.CreateNetwork(784, 200, 10, 0.1)
```

This creates:
- Hidden weights matrix: 200×784 (200 neurons, 784 inputs each)
- Output weights matrix: 10×200 (10 neurons, 200 inputs each)
- Weights are randomly initialized using uniform distribution

### Training a single example

```go
net.Train(inputs, targets)
```

This performs:
1. **Forward propagation**: Calculate hidden and output activations
2. **Error calculation**: Compare outputs to targets
3. **Backpropagation**: Update weights using gradient descent

### Making a prediction

```go
outputs := net.Predict(inputs)
```

This runs forward propagation only, returning the network's confidence for each digit.

## Next steps

<CardGroup cols={2}>
  <Card title="API Reference" icon="book" href="/api-reference">
    Explore the full Network API and helper functions
  </Card>
  <Card title="Advanced Usage" icon="graduation-cap" href="/advanced">
    Learn about tuning hyperparameters and improving accuracy
  </Card>
</CardGroup>

## Tips for best results

<Steps>
  <Step title="Image quality">
    Use clear, centered digits with good contrast for better predictions
  </Step>
  
  <Step title="Training time">
    More epochs generally improve accuracy, but 5 epochs is usually sufficient
  </Step>
  
  <Step title="Learning rate">
    The default 0.1 learning rate works well, but you can experiment by modifying the code
  </Step>
</Steps>
